{
"root_page":{
"children":[
{
"children":[],
"id":"31e3c712b4ce4f16bb8e5d352e76fb0e",
"paragraphs":[
{
"fragments":[
{
"text":"Before optimizing something for speed it is important to measure how long it takes. We are interested in optimizing the time it takes for RLMeta to compile itself.",
"type":"text"
}
],
"id":"521b7a601c67426c8f6bd298c13bd93e",
"type":"text"
},
{
"fragments":[
{
"text":"RLMeta compiles itself in the following steps:",
"type":"text"
}
],
"id":"985edf70d12a484ca84294b8c14ba294",
"type":"text"
},
{
"child_type":"unordered",
"children":[
{
"child_type":null,
"children":[],
"fragments":[
{
"text":"Generate support libraries",
"type":"text"
}
]
},
{
"child_type":null,
"children":[],
"fragments":[
{
"text":"Compile parser",
"type":"text"
}
]
},
{
"child_type":null,
"children":[],
"fragments":[
{
"text":"Compile code generator",
"type":"text"
}
]
},
{
"child_type":null,
"children":[],
"fragments":[
{
"text":"Assemble the pieces into a Python file",
"type":"text"
}
]
}
],
"id":"a34186a39a1b4fc5bff2e511fe90e5b5",
"type":"list"
},
{
"fragments":[
{
"text":"The ",
"type":"text"
},
{
"text":"compile.sh",
"type":"code"
},
{
"text":" script performs those steps like follows:",
"type":"text"
}
],
"id":"f7f26c78660b4f3689dd2b45e5576d62",
"type":"text"
},
{
"chunkpath":[],
"filepath":[
"compile.sh"
],
"fragments":[
{
"text":"#!/bin/bash\n\nset -e\n\nrlmeta_compiler=\"$(pwd)/$1\"\n\ncd \"$(dirname \"$0\")\"\n\nto_python_string() {\n    python -c 'import sys; sys.stdout.write(repr(sys.stdin.read()))'\n}\n\nsupport_py_string=$(to_python_string < support.py)\nsupport_py=$(python \"$rlmeta_compiler\" --support)\nparser_py=$(python \"$rlmeta_compiler\" < parser.rlmeta)\ncodegenerator_py=$(python \"$rlmeta_compiler\" < codegenerator.rlmeta)\n\ncat <<EOF\n",
"type":"code"
},
{
"blank_lines_before":0,
"path":[
"python template"
],
"prefix":"",
"type":"chunk"
},
{
"text":"EOF\n",
"type":"code"
}
],
"id":"6299de61907642ed92cffaa3764f2c61",
"type":"code"
},
{
"fragments":[
{
"text":"The Python file template rendered at the end of the script looks like this:",
"type":"text"
}
],
"id":"5a1ea51cecd24d2c908b25e932c22782",
"type":"text"
},
{
"chunkpath":[
"python template"
],
"filepath":[
"compile.sh"
],
"fragments":[
{
"text":"import sys\n\nSUPPORT = $support_py_string\n\n$support_py\n\n$parser_py\n\n$codegenerator_py\n\njoin = \"\".join\n\ndef compile_grammar(grammar):\n    parser = Parser()\n    code_generator = CodeGenerator()\n    return code_generator.run(\"ast\", parser.run(\"grammar\", grammar))\n\nif __name__ == \"__main__\":\n    if \"--support\" in sys.argv:\n        sys.stdout.write(SUPPORT)\n    else:\n        try:\n            sys.stdout.write(compile_grammar(sys.stdin.read()))\n        except _MatchError as e:\n            sys.stderr.write(e.describe())\n            sys.exit(1)\n",
"type":"code"
}
],
"id":"2b5a39bd729f4463b703b7647220596e",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"To measure how long it takes for RLMeta to compile itself, we use the following command:",
"type":"text"
}
],
"id":"0e17cb244d5c44559c5a9e8043c77652",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.756s\nuser\t0m0.701s\nsys\t0m0.054s\n",
"type":"code"
}
],
"id":"7353abcf68334f5cb322813d3cc21d15",
"type":"code"
},
{
"fragments":[
{
"text":"It takes ",
"type":"text"
},
{
"text":"0.756s",
"type":"strong"
},
{
"text":". The goal of our optimizations is to decrease this time.",
"type":"text"
}
],
"id":"572fc8860696403f951de597e21532ea",
"type":"text"
},
{
"fragments":[
{
"text":"To get a better understanding of what takes time, we measure how long it takes to compile the parser and the code generator individually:",
"type":"text"
}
],
"id":"56a345b93d2d48b89c310eb50d21dc0f",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time python rlmeta.py < parser.rlmeta > /dev/null\n\nreal\t0m0.380s\nuser\t0m0.363s\nsys\t0m0.015s\n",
"type":"code"
}
],
"id":"3e4ed2e3741a4adcaf053218c7aff51c",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time python rlmeta.py < codegenerator.rlmeta > /dev/null\n\nreal\t0m0.351s\nuser\t0m0.331s\nsys\t0m0.019s\n",
"type":"code"
}
],
"id":"4fa0e0e48b4844c496c1a8ad71f6b1ee",
"type":"code"
},
{
"fragments":[
{
"text":"Both grammars take roughly the same time to compile (",
"type":"text"
},
{
"text":"0.380s",
"type":"strong"
},
{
"text":" and ",
"type":"text"
},
{
"text":"0.351s",
"type":"strong"
},
{
"text":"). Moreover, the compilation of the grammars take up most of the time. The rest (generating the support libraries and rendering the Python file template) takes only ",
"type":"text"
},
{
"text":"0.025s",
"type":"strong"
},
{
"text":" (0.756-0.380-0.351). We will therefore focus on making compilation of grammars faster.",
"type":"text"
}
],
"id":"5bace825a3ee46348cd0c8422ea89d8c",
"type":"text"
},
{
"fragments":[
{
"text":"To get a better understanding of what takes time when compiling grammars, we compile the parser and the code generator with the Python profiler:",
"type":"text"
}
],
"id":"40b668292c97425c9fe0473a1499dca8",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n             513780 function calls (430760 primitive calls) in 0.476 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   8182/2    0.053    0.000    0.427    0.213 rlmeta.py:7(_or)\n    22758    0.040    0.000    0.106    0.000 rlmeta.py:234(next)\n    17411    0.027    0.000    0.046    0.000 rlmeta.py:203(fail)\n  26658/2    0.023    0.000    0.427    0.213 rlmeta.py:16(_and)\n  10730/2    0.020    0.000    0.427    0.213 rlmeta.py:43(_match_rule)\n     2177    0.019    0.000    0.019    0.000 rlmeta.py:147(write)\n        1    0.013    0.013    0.013    0.013 {method 'write' of 'file' objects}\n...\n",
"type":"code"
}
],
"id":"020968a024554716aa6db67d511fd04e",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < codegenerator.rlmeta\n...\n         450885 function calls (377441 primitive calls) in 0.433 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   8152/2    0.052    0.000    0.396    0.198 rlmeta.py:7(_or)\n    19754    0.046    0.000    0.097    0.000 rlmeta.py:234(next)\n    15838    0.026    0.000    0.049    0.000 rlmeta.py:203(fail)\n  23478/2    0.020    0.000    0.396    0.198 rlmeta.py:16(_and)\n   9883/2    0.018    0.000    0.396    0.198 rlmeta.py:43(_match_rule)\n    15838    0.016    0.000    0.016    0.000 rlmeta.py:211(__init__)\n        1    0.012    0.012    0.012    0.012 {method 'write' of 'file' objects}\n...\n",
"type":"code"
}
],
"id":"77424da4068d4cba8d77373149853250",
"type":"code"
},
{
"fragments":[
{
"text":"This again shows that both grammars take roughly the same time to compile (",
"type":"text"
},
{
"text":"0.476s",
"type":"strong"
},
{
"text":" and ",
"type":"text"
},
{
"text":"0.433s",
"type":"strong"
},
{
"text":"). I'm not sure why these numbers are higher than the ",
"type":"text"
},
{
"text":"time",
"type":"code"
},
{
"text":" measurements. Perhaps because profiling is turned on. It also shows what functions take most time. Both grammars have roughly the same functions at the top of the list: ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":", ",
"type":"text"
},
{
"text":"_next",
"type":"code"
},
{
"text":", ",
"type":"text"
},
{
"text":"fail",
"type":"code"
},
{
"text":", ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":", ",
"type":"text"
},
{
"text":"_match_rule",
"type":"code"
},
{
"text":", ",
"type":"text"
},
{
"text":"write",
"type":"code"
},
{
"text":". Since the profiling output looks similar for the two grammars, we will only profile compilation of the parser.",
"type":"text"
}
],
"id":"55354fb22db94925a6774620ca225a57",
"type":"text"
},
{
"fragments":[
{
"text":"Let's examine the ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" function since most time is spend there and see if we can make it any faster.",
"type":"text"
}
],
"id":"66d646055a1f4012926e0859ac282ad5",
"type":"text"
}
],
"title":"What to measure?"
},
{
"children":[],
"id":"2ac127d411b749919b45ae11c1e47975",
"paragraphs":[
{
"fragments":[
{
"text":"The ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" function looks like this:",
"type":"text"
}
],
"id":"b7784aca13ae469090a01f2e848e1072",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"def _or(self, matchers):\n    original_stream = self._stream\n    for matcher in matchers:\n        try:\n            return matcher()\n        except _MatchError:\n            self._stream = original_stream\n    original_stream.fail(\"no choice matched\")\n",
"type":"code"
}
],
"id":"fbb934f6ca3a48c2a34ff06e790937f6",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"There is no obvious way to optimize it so instead we try to reduce the number of calls to it. The code generator always generates calls to ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" even if there is only one choice: ",
"type":"text"
},
{
"text":"self._or([matcher])",
"type":"code"
},
{
"text":". In this case the call to ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" is unnecessary because the single matcher can be called directly: ",
"type":"text"
},
{
"text":"matcher()",
"type":"code"
},
{
"text":". The same reasoning goes for ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":".",
"type":"text"
}
],
"id":"bd82c8729bf44697a4b5df1236c536e3",
"type":"text"
},
{
"fragments":[
{
"text":"To make the code generator skip the calls to ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" and ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":", we add the following cases to it:",
"type":"text"
}
],
"id":"f04c0be1594f45cf984a4d50f14c5953",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"| [\"Or\" ast:x]  -> x\n| [\"And\" ast:x] -> x\n",
"type":"code"
}
],
"id":"273e39ffd9ef4e7999674677fed06fdc",
"language":"rlmeta",
"type":"code"
},
{
"fragments":[
{
"text":"If it sees an ",
"type":"text"
},
{
"text":"Or",
"type":"code"
},
{
"text":" or an ",
"type":"text"
},
{
"text":"And",
"type":"code"
},
{
"text":" node with only one child, it will skip the calls to ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" and ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":" and just generate the child matcher.",
"type":"text"
}
],
"id":"14dc897e3f4e43b8b97b12bcd8f073b6",
"type":"text"
},
{
"fragments":[
{
"text":"We create a new version of RLMeta as described in ",
"type":"text"
},
{
"text":"Modifying the RLMeta metacompiler",
"type":"link",
"url":"/writing/modifying-rlmeta/"
},
{
"text":" and then we measure.",
"type":"text"
}
],
"id":"6b45bd89bb7b484488730d74ac208ded",
"type":"text"
},
{
"fragments":[
{
"text":"Unfortunately, this does not seem to have any effect on the speed. Why is that? Perhaps because the two added cases to the code generator also makes it slower.",
"type":"text"
}
],
"id":"df0b7ed6167e4139a369016bb38d789e",
"type":"text"
},
{
"fragments":[
{
"text":"We discard this change for now and move on to the ",
"type":"text"
},
{
"text":"next",
"type":"code"
},
{
"text":" function which is second on the list of time consuming functions.",
"type":"text"
}
],
"id":"c62d17cbff144e5f83f3e1aec924c3da",
"type":"text"
}
],
"title":"And/Or optimization"
},
{
"children":[],
"id":"f4741ec692124b31af139231151e3a29",
"paragraphs":[
{
"fragments":[
{
"text":"The ",
"type":"text"
},
{
"text":"next",
"type":"code"
},
{
"text":" function looks like this:",
"type":"text"
}
],
"id":"c776771e7e24443182189d40f5bcda8f",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"def next(self):\n    if self.is_at_end():\n        self.fail(\"not eof\")\n    next_object = self._objects[0]\n    return (\n        next_object,\n        self._advance(next_object, self._objects[1:]),\n    )\n",
"type":"code"
}
],
"id":"431199a71a704d759438687a0d02dbc8",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"Every time it is called, which is once per object in the input stream, the input stream is sliced: ",
"type":"text"
},
{
"text":"self._objects[1:]",
"type":"code"
},
{
"text":". It creates a new list with all objects except the first. For example, if the input stream is the string \"print(1)\", it will create the substrings \"rint(1)\", \"int(1)\", \"nt(1)\", and so on. This is wasteful.",
"type":"text"
}
],
"id":"80e0c6d0e6d1490eb4e7eb29bab162df",
"type":"text"
},
{
"fragments":[
{
"text":"To avoid slicing the input stream we rewrite input streams to instead maintain an index where threy're at. The next object is gotten by indexing the array, and the rest of the objects no longer has to be computed. The index is just incremented instead. (The complete diff can be ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/a358b1921a5f9afae769512ca27db795af947648#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":").",
"type":"text"
}
],
"id":"5ba7aafaf41a430ba5b58eaeafc1ab4f",
"type":"text"
},
{
"fragments":[
{
"text":"With this change, the ",
"type":"text"
},
{
"text":"next",
"type":"code"
},
{
"text":" method looks like this:",
"type":"text"
}
],
"id":"a2623d33daf946e5ab8404c4a3c0c9fa",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"def next(self):\n    if self.is_at_end():\n        self.fail(\"not eof\")\n    return (self._objects[self._index], self._advance())\n",
"type":"code"
}
],
"id":"0fc87189a13b4d9cb5b574f1cb0c95ea",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"We create a new version of RLMeta and then we measure:",
"type":"text"
}
],
"id":"4000afeb3a0348cb871af30f5d6d738a",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.608s\nuser\t0m0.585s\nsys\t0m0.022s\n",
"type":"code"
}
],
"id":"2622660f128446a2b8a9fdd37f09e92c",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         513780 function calls (430760 primitive calls) in 0.415 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   8182/2    0.042    0.000    0.366    0.183 rlmeta.py:7(_or)\n    17411    0.026    0.000    0.044    0.000 rlmeta.py:203(fail)\n  26658/2    0.021    0.000    0.366    0.183 rlmeta.py:16(_and)\n  10730/2    0.019    0.000    0.366    0.183 rlmeta.py:43(_match_rule)\n     2177    0.019    0.000    0.019    0.000 rlmeta.py:147(write)\n    22758    0.017    0.000    0.077    0.000 rlmeta.py:235(next)\n        1    0.014    0.014    0.014    0.014 {method 'write' of 'file' objects}\n...\n",
"type":"code"
}
],
"id":"6a334e88422c479fb696729a3a7f1ed2",
"type":"code"
},
{
"fragments":[
{
"text":"The overall compilation time went from ",
"type":"text"
},
{
"text":"0.756s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.608s",
"type":"strong"
},
{
"text":" (",
"type":"text"
},
{
"text":"~20%",
"type":"strong"
},
{
"text":" faster). The compilation time for a single grammar went from ",
"type":"text"
},
{
"text":"0.476s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.415s",
"type":"strong"
},
{
"text":" (",
"type":"text"
},
{
"text":"~12%",
"type":"strong"
},
{
"text":" faster). The ",
"type":"text"
},
{
"text":"next",
"type":"code"
},
{
"text":" method moved down the list from ",
"type":"text"
},
{
"text":"0.040s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.017s",
"type":"strong"
},
{
"text":".",
"type":"text"
}
],
"id":"b63692cca6ac4450b7d52a79c5ef3746",
"type":"text"
},
{
"fragments":[
{
"text":"Finally some progress.",
"type":"text"
}
],
"id":"90a6d8d35c624cc9af3e7c2fd4f9d26c",
"type":"text"
},
{
"fragments":[
{
"text":"Still not sure what to do with ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":"/",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":". Examine ",
"type":"text"
},
{
"text":"fail",
"type":"code"
},
{
"text":" and see that there is no obvious optimization. Same for ",
"type":"text"
},
{
"text":"_match_call_rule",
"type":"code"
},
{
"text":". Let's move on to ",
"type":"text"
},
{
"text":"write",
"type":"code"
},
{
"text":".",
"type":"text"
}
],
"id":"98d7e7a9360c4481a94567fba763def7",
"type":"text"
}
],
"title":"Avoid slicing input"
},
{
"children":[],
"id":"8deb8e7f493a474d8555eb7db34a89dc",
"paragraphs":[
{
"fragments":[
{
"text":"The ",
"type":"text"
},
{
"text":"write",
"type":"code"
},
{
"text":" method of the ",
"type":"text"
},
{
"text":"_Output",
"type":"code"
},
{
"text":" class looks like this:",
"type":"text"
}
],
"id":"7f8a065809ef4734bee1424e5afca6a6",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"class _Output(object):\n\n    def __init__(self):\n        self.value = \"\"\n        self.indentation = 0\n\n    def write(self, value):\n        for ch in value:\n            if self.value and ch != \"\\n\" and self.value[-1] == \"\\n\":\n                self.value += \"    \"*self.indentation\n            self.value += ch\n",
"type":"code"
}
],
"id":"351831073afe4f19976c736abcb9739a",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"It builds up the value by concatenating strings to the current value: ",
"type":"text"
},
{
"text":"self.value += ..",
"type":"code"
},
{
"text":". This concatenation is done once per generated character. Concatenating strings in Python this way is not the fastest way to do it.",
"type":"text"
}
],
"id":"84c5b2ac6942424e9ad8cf0b6122170e",
"type":"text"
},
{
"fragments":[
{
"text":"A better way is to use the ",
"type":"text"
},
{
"text":"StringIO",
"type":"code"
},
{
"text":" class. Its ",
"type":"text"
},
{
"text":"write",
"type":"code"
},
{
"text":" method is used to build up the value, and its ",
"type":"text"
},
{
"text":"getvalue",
"type":"code"
},
{
"text":" method is used to return it:",
"type":"text"
}
],
"id":"c69d698ef693433e9a72a88ddf466668",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"class _Output(object):\n\n    def __init__(self):\n        self.buffer = StringIO()\n        self.indentation = 0\n        self.on_newline = True\n\n    @property\n    def value(self):\n        return self.buffer.getvalue()\n\n    def write(self, value):\n        for ch in value:\n            is_linebreak = ch == \"\\n\"\n            if self.indentation and self.on_newline and not is_linebreak:\n                self.buffer.write(\"    \"*self.indentation)\n            self.buffer.write(ch)\n            self.on_newline = is_linebreak\n",
"type":"code"
}
],
"id":"d0de1fa5579d488e8a27cc36443fcc64",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"The complete diff can be shown ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/d5327b86d3de97a621f9ca0a8d11e445484d0fcb#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":".",
"type":"text"
}
],
"id":"33c01c5feb944e2080fe3e440c249405",
"type":"text"
},
{
"fragments":[
{
"text":"We create a new version of RLMeta and then we measure:",
"type":"text"
}
],
"id":"da9f1b29fab24411a0c58fa35a8944d4",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.603s\nuser\t0m0.564s\nsys\t0m0.038s\n",
"type":"code"
}
],
"id":"6d47ef329bf24092879aac74e86e34b9",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         527910 function calls (444890 primitive calls) in 0.397 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   8182/2    0.042    0.000    0.370    0.185 rlmeta.py:12(_or)\n    17411    0.026    0.000    0.045    0.000 rlmeta.py:215(fail)\n  26658/2    0.021    0.000    0.370    0.185 rlmeta.py:21(_and)\n  10730/2    0.019    0.000    0.370    0.185 rlmeta.py:48(_match_rule)\n    22758    0.017    0.000    0.078    0.000 rlmeta.py:247(next)\n    17408    0.012    0.000    0.018    0.000 rlmeta.py:279(__init__)\n...\n     2177    0.006    0.000    0.009    0.000 rlmeta.py:157(write)\n...\n",
"type":"code"
}
],
"id":"41a51526c38b44e992df8bc05b39a404",
"type":"code"
},
{
"fragments":[
{
"text":"The overall compilation time went from ",
"type":"text"
},
{
"text":"0.608s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.603s",
"type":"strong"
},
{
"text":". The compilation time for a single grammar went from ",
"type":"text"
},
{
"text":"0.415s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.397s",
"type":"strong"
},
{
"text":". This is not significant. But much less time is spent in the ",
"type":"text"
},
{
"text":"write",
"type":"code"
},
{
"text":" method (",
"type":"text"
},
{
"text":"0.006s",
"type":"strong"
},
{
"text":" compared to ",
"type":"text"
},
{
"text":"0.019s",
"type":"strong"
},
{
"text":") so we keep this change.",
"type":"text"
}
],
"id":"d07638f612804b77bd8f62fc3fe6db72",
"type":"text"
},
{
"fragments":[
{
"text":"It might be that some more time is spent doing the ",
"type":"text"
},
{
"text":"getvalue",
"type":"code"
},
{
"text":" call. But it does not show up at the top of the list.",
"type":"text"
}
],
"id":"adcddf1005a74fd7a9ee09d837cb4fac",
"type":"text"
},
{
"fragments":[
{
"text":"The ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" and ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":" functions are still high up on the list. Now that we have made other parts of RLMeta faster, will the and/or optimization be more useful?",
"type":"text"
}
],
"id":"99c7f43fb54844afa4f132a4cb869458",
"type":"text"
}
],
"title":"Faster string concatenation"
},
{
"children":[],
"id":"006ce805fef64eb88c4b8214d7e85495",
"paragraphs":[
{
"fragments":[
{
"text":"We apply the and/or optimization again, create a new version of RLMeta and then we measure:",
"type":"text"
}
],
"id":"4a564795514347da8f9dbd5c9fc03018",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.592s\nuser\t0m0.560s\nsys\t0m0.031s\n",
"type":"code"
}
],
"id":"831f44f667f14625bf32fca1f8f5b57b",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         508708 function calls (437696 primitive calls) in 0.380 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n 4898/230    0.039    0.000    0.351    0.002 rlmeta.py:12(_or)\n    15852    0.025    0.000    0.041    0.000 rlmeta.py:215(fail)\n  10620/2    0.019    0.000    0.354    0.177 rlmeta.py:48(_match_rule)\n  21301/2    0.018    0.000    0.354    0.177 rlmeta.py:21(_and)\n    23516    0.018    0.000    0.078    0.000 rlmeta.py:247(next)\n    19154    0.013    0.000    0.020    0.000 rlmeta.py:279(__init__)\n   8323/1    0.012    0.000    0.184    0.184 rlmeta.py:93(_match_list)\n    12847    0.010    0.000    0.024    0.000 rlmeta.py:290(_advance)\n     6613    0.010    0.000    0.052    0.000 rlmeta.py:79(_match_charseq)\n...\n",
"type":"code"
}
],
"id":"e91a744e842d49f896706bfbca4c4a75",
"type":"code"
},
{
"fragments":[
{
"text":"The overall compilation time went from ",
"type":"text"
},
{
"text":"0.603s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.592s",
"type":"strong"
},
{
"text":". The compilation time for a single grammar went from ",
"type":"text"
},
{
"text":"0.397s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.380s",
"type":"strong"
},
{
"text":". This is not significant, but less time is spend int ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" and ",
"type":"text"
},
{
"text":"_and",
"type":"code"
},
{
"text":", so we keep this change now.",
"type":"text"
}
],
"id":"dfdab4b9190c4b2d80ed922ffddd395c",
"type":"text"
},
{
"fragments":[
{
"text":"We previously observed that writing output took some time. We made the write method faster. Now we will try another change that will cause less characters to be written to the output. This will change the grammar, and not just support libraries.",
"type":"text"
}
],
"id":"3578213d608b451eb478bb241f2217b3",
"type":"text"
}
],
"title":"And/Or again"
},
{
"children":[],
"id":"a06ba4a722cc461f9f3eabe2c0f60e53",
"paragraphs":[
{
"fragments":[
{
"text":"The main part of the code generator has two rules that generate code for ast nodes:",
"type":"text"
}
],
"id":"c93daaec0e1745f0a301df283b259d92",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"CodeGenerator {\n  ast =\n    | [\"Grammar\" .:x ast*:ys] -> { \"class \" x \"(_Grammar):\\n\" > ys < }\n    ...\n    | astFnBody:x             -> { \"(lambda:\\n\" > x < \"\\n)\" }\n  astFnBody =\n    | [\"Or\" astItems:x]       -> { \"self._or([\" x \"])\"               }\n    ...\n  ...\n}\n",
"type":"code"
}
],
"id":"a9c5bf6ea32743c9b1205c8ee8905109",
"language":"rlmeta",
"type":"code"
},
{
"fragments":[
{
"text":"If an ast node's body should be wrapped in a lambda, it is put in the ",
"type":"text"
},
{
"text":"astFnBody",
"type":"code"
},
{
"text":" rule and the lambda is generated by the last choice in the ",
"type":"text"
},
{
"text":"ast",
"type":"code"
},
{
"text":" rule. The lambda is generated with newlines and indentation like this:",
"type":"text"
}
],
"id":"c63b2dccbca04e86b2f5d4284ca7b15a",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"(lambda:\n    body\n)\n",
"type":"code"
}
],
"id":"61a73f2758ad4a3983817fb1f723132a",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"The newlines and indentation is not necessary. It might make the generated code easier to the read at the expense of outputting at least 6 more characters (two newlines and at least 4 spaces of indent). Instead we could generate this:",
"type":"text"
}
],
"id":"bd96ddc557c646a189bb472505c0c3b9",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"(lambda: body)\n",
"type":"code"
}
],
"id":"50b70f0e04ed450495685ee37d8ee7a1",
"language":"python",
"type":"code"
},
{
"fragments":[
{
"text":"We rewrite the code generator like this:",
"type":"text"
}
],
"id":"427013a8f3994fd592f1e57a48f66f63",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"CodeGenerator {\n  ast =\n    | [\"Grammar\" .:x ast*:ys] -> { \"class \" x \"(_Grammar):\\n\" > ys < }\n    ...\n    | [\"Or\" astItems:x]       -> { \"(lambda: self._or([\" x \"]))\"     }\n    ...\n  ...\n}\n",
"type":"code"
}
],
"id":"b92bc3eb6f294076b7f9953489ab1b5a",
"language":"rlmeta",
"type":"code"
},
{
"fragments":[
{
"text":"The complete diff can be shown ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/496c98d367a5dce2def17680d92f095cd2c274a5#diff-14e1afbb5e4a4c88a29fd7256cd350fb"
},
{
"text":".",
"type":"text"
}
],
"id":"bcfa7fa533ec4d099365cce755c0315b",
"type":"text"
},
{
"fragments":[
{
"text":"This also removes the ",
"type":"text"
},
{
"text":"astFnBody",
"type":"code"
},
{
"text":" rule in favor of duplicating the lambda output code. This also lays the groundwork to enable the next optimization.",
"type":"text"
}
],
"id":"7ac771e9267c427d9acc01b8bad8494b",
"type":"text"
},
{
"fragments":[
{
"text":"We create a new version of RLMeta and then we measure:",
"type":"text"
}
],
"id":"871da1d7d52945cf9a028868932312a3",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.565s\nuser\t0m0.530s\nsys\t0m0.034s\n",
"type":"code"
}
],
"id":"8cdfc1e73a1646a59b4efde7495ce4fb",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         457174 function calls (396876 primitive calls) in 0.343 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n 4518/230    0.039    0.000    0.343    0.001 rlmeta.py:12(_or)\n    15769    0.024    0.000    0.040    0.000 rlmeta.py:215(fail)\n  10240/2    0.019    0.000    0.347    0.174 rlmeta.py:48(_match_rule)\n    23516    0.017    0.000    0.078    0.000 rlmeta.py:247(next)\n  20921/2    0.017    0.000    0.347    0.174 rlmeta.py:21(_and)\n    19154    0.014    0.000    0.020    0.000 rlmeta.py:279(__init__)\n   8323/1    0.012    0.000    0.176    0.176 rlmeta.py:93(_match_list)\n     6613    0.010    0.000    0.052    0.000 rlmeta.py:79(_match_charseq)\n    12847    0.010    0.000    0.024    0.000 rlmeta.py:290(_advance)\n    27805    0.010    0.000    0.010    0.000 rlmeta.py:239(__init__)\n    17623    0.010    0.000    0.010    0.000 {method 'format' of 'str' objects}\n...\n",
"type":"code"
}
],
"id":"5435a568ac1543cab4c2287c39f4dc74",
"type":"code"
},
{
"fragments":[
{
"text":"The overall compilation time went from ",
"type":"text"
},
{
"text":"0.592s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.565s",
"type":"strong"
},
{
"text":". The compilation time for a single grammar went from ",
"type":"text"
},
{
"text":"0.380s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.343s",
"type":"strong"
},
{
"text":".",
"type":"text"
}
],
"id":"c837c11677a5464b9537266d0e6fd08e",
"type":"text"
},
{
"fragments":[
{
"text":"Armed with a new mindset we think about how another optimization that changes the grammar.",
"type":"text"
}
],
"id":"fd39dbcf20b2476fbe0b7755202c77a7",
"type":"text"
}
],
"title":"Collapse and remove newlines/indents"
},
{
"children":[],
"id":"e7f710aa99bc4a7c895e905f60b4beb8",
"paragraphs":[
{
"fragments":[
{
"text":"Can reduce a whole lot of ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" calls.",
"type":"text"
}
],
"id":"b5f4f2ebe7cd4ec3bd630370d2afb036",
"type":"text"
},
{
"fragments":[
{
"text":"Motivation is to get rid of ors in code generator.",
"type":"text"
}
],
"id":"34855f08881448499205cbcdbe4e7bdd",
"type":"text"
},
{
"fragments":[
{
"text":"The complete diff can be shown viewed online ",
"type":"text"
},
{
"text":"here",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/61ff2132a057cf986ae0eae8575d24b066ab3e43"
},
{
"text":" and ",
"type":"text"
},
{
"text":"here",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/f054fdca6c939506e2820d86d466fe1763244c43#diff-14e1afbb5e4a4c88a29fd7256cd350fb"
},
{
"text":".",
"type":"text"
}
],
"id":"991870bc9fdb41b0bd564d68ce5f1bea",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.406s\nuser\t0m0.373s\nsys\t0m0.032s\n",
"type":"code"
}
],
"id":"cfe430e9506f423a8a5f70d3019da5ba",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         274963 function calls (238101 primitive calls) in 0.220 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n  11129/2    0.021    0.000    0.215    0.107 rlmeta.py:48(_match_rule)\n 4177/252    0.020    0.000    0.209    0.001 rlmeta.py:12(_or)\n     8460    0.013    0.000    0.022    0.000 rlmeta.py:219(fail)\n     7021    0.011    0.000    0.056    0.000 rlmeta.py:79(_match_charseq)\n   9010/2    0.010    0.000    0.215    0.107 rlmeta.py:21(_and)\n     9099    0.009    0.000    0.020    0.000 rlmeta.py:269(_advance)\n    10540    0.009    0.000    0.039    0.000 rlmeta.py:251(next)\n     9100    0.008    0.000    0.011    0.000 rlmeta.py:261(__init__)\n    12801    0.006    0.000    0.006    0.000 {method 'format' of 'str' objects}\n     1443    0.006    0.000    0.008    0.000 rlmeta.py:161(write)\n    26070    0.005    0.000    0.005    0.000 rlmeta.py:266(position)\n     8460    0.005    0.000    0.005    0.000 rlmeta.py:227(__init__)\n    10908    0.005    0.000    0.005    0.000 rlmeta.py:243(__init__)\n...\n",
"type":"code"
}
],
"id":"99fb90ddb610472ca4c6f860db393ad4",
"type":"code"
},
{
"fragments":[
{
"text":"The overall compilation time went from ",
"type":"text"
},
{
"text":"0.565s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.406s",
"type":"strong"
},
{
"text":". The compilation time for a single grammar went from ",
"type":"text"
},
{
"text":"0.343s",
"type":"strong"
},
{
"text":" to ",
"type":"text"
},
{
"text":"0.220s",
"type":"strong"
},
{
"text":". Real performance gain! ",
"type":"text"
},
{
"text":"_or",
"type":"code"
},
{
"text":" is kicked down to the second position.",
"type":"text"
}
],
"id":"aab6484037fe44589bde970ba5e725c0",
"type":"text"
}
],
"title":"Match call rule"
},
{
"children":[],
"id":"5160cbceeb9f45bb992c65cc688c3c18",
"paragraphs":[
{
"fragments":[
{
"text":"This was not really significant.",
"type":"text"
}
],
"id":"3f4185deb25b4f08a6b167b25844871f",
"type":"text"
}
],
"title":"Don't memoize code generator"
},
{
"children":[],
"id":"f0338cf15ddb49099abf6a135a0961d7",
"paragraphs":[
{
"fragments":[
{
"text":"The complete diff can be shown ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/3cbf3437cf0478bd899ec6c6c839b3f07c32f86b#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":".",
"type":"text"
}
],
"id":"aa591747e68c4049a220cf9c5e382e99",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.392s\nuser\t0m0.364s\nsys\t0m0.026s\n",
"type":"code"
}
],
"id":"d618172eff37461e9be22d1d2b3d02a4",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         274963 function calls (238101 primitive calls) in 0.220 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n 4177/252    0.020    0.000    0.206    0.001 rlmeta.py:12(_or)\n  11129/2    0.020    0.000    0.212    0.106 rlmeta.py:48(_match_rule)\n     8460    0.014    0.000    0.022    0.000 rlmeta.py:219(fail)\n     7021    0.012    0.000    0.057    0.000 rlmeta.py:79(_match_charseq)\n   9010/2    0.010    0.000    0.212    0.106 rlmeta.py:21(_and)\n     9099    0.009    0.000    0.021    0.000 rlmeta.py:269(_advance)\n    10540    0.009    0.000    0.039    0.000 rlmeta.py:251(next)\n     9100    0.008    0.000    0.011    0.000 rlmeta.py:261(__init__)\n    12801    0.006    0.000    0.006    0.000 {method 'format' of 'str' objects}\n     1443    0.006    0.000    0.009    0.000 rlmeta.py:161(write)\n...\n",
"type":"code"
}
],
"id":"6f1b72363ee8429ca4c55b24fcc1be12",
"type":"code"
},
{
"fragments":[
{
"text":"Slightly better.",
"type":"text"
}
],
"id":"d89cfb31abe04f36abc33dab1572f62e",
"type":"text"
}
],
"title":"Optimize position"
},
{
"children":[],
"id":"a5f5f3907de04b28bdf52a8ba57b4a71",
"paragraphs":[
{
"fragments":[
{
"text":"The complete diff can be shown viewed online ",
"type":"text"
},
{
"text":"here",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/d32dfb7235b2d1353d8045dc1999a6dc07f493bf#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":" and ",
"type":"text"
},
{
"text":"here",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/dd3d3b8c35744b6d5e5ef42dbd46aafa9e1331ad#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":".",
"type":"text"
}
],
"id":"33ef278f33554dd3b9d9d5dbb24cd12a",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.370s\nuser\t0m0.336s\nsys\t0m0.034s\n",
"type":"code"
}
],
"id":"a21c297ab39041ee83df0f602e95e037",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         268387 function calls (231525 primitive calls) in 0.204 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n 4177/252    0.020    0.000    0.194    0.001 rlmeta.py:12(_or)\n  11129/2    0.019    0.000    0.199    0.100 rlmeta.py:48(_match_rule)\n     8460    0.013    0.000    0.021    0.000 rlmeta.py:219(fail)\n     7021    0.010    0.000    0.050    0.000 rlmeta.py:79(_match_charseq)\n     9099    0.010    0.000    0.021    0.000 rlmeta.py:269(_advance)\n   9010/2    0.009    0.000    0.199    0.100 rlmeta.py:21(_and)\n    10540    0.008    0.000    0.038    0.000 rlmeta.py:251(next)\n     9100    0.007    0.000    0.011    0.000 rlmeta.py:261(__init__)\n     8460    0.006    0.000    0.006    0.000 rlmeta.py:227(__init__)\n     1443    0.005    0.000    0.008    0.000 rlmeta.py:161(write)\n    10908    0.004    0.000    0.004    0.000 rlmeta.py:243(__init__)\n     8460    0.004    0.000    0.025    0.000 rlmeta.py:248(fail)\n    10993    0.004    0.000    0.005    0.000 rlmeta.py:256(is_at_end)\n    885/5    0.004    0.000    0.199    0.040 rlmeta.py:27(_star)\n 3274/747    0.004    0.000    0.005    0.000 rlmeta.py:141(create)\n    12616    0.003    0.000    0.003    0.000 {method 'write' of 'cStringIO.StringO' objects}\n    17881    0.002    0.000    0.002    0.000 rlmeta.py:266(position)\n     1802    0.002    0.000    0.014    0.000 rlmeta.py:59(_match_range)\n     6225    0.002    0.000    0.002    0.000 {method 'format' of 'str' objects}\n...\n",
"type":"code"
}
],
"id":"d924bf5c2ea4444fa9cb3afc812ae719",
"type":"code"
},
{
"fragments":[
{
"text":"During search we found a few super optimizations and a few less super once. We will stop here.",
"type":"text"
}
],
"id":"002e03eb44d448db865027fec5b771c2",
"type":"text"
}
],
"title":"Make fail messages lazy"
},
{
"children":[],
"id":"898ec92a13304692857a6d4de445e4d2",
"paragraphs":[
{
"fragments":[
{
"text":"The complete diff can be shown ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/6bcd1457c0fc4726c0d7bc5020a77dcef7f05ed3#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":".",
"type":"text"
}
],
"id":"384cd111a3bd4a20a895766d215d655c",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.351s\nuser\t0m0.319s\nsys\t0m0.031s\n",
"type":"code"
}
],
"id":"291c53933d4645f884397d8c102c3d4d",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         248506 function calls (211644 primitive calls) in 0.187 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n  11129/2    0.019    0.000    0.182    0.091 rlmeta.py:48(_match_rule)\n 4177/252    0.018    0.000    0.177    0.001 rlmeta.py:12(_or)\n     8460    0.013    0.000    0.022    0.000 rlmeta.py:221(fail)\n     7021    0.010    0.000    0.037    0.000 rlmeta.py:79(_match_charseq)\n   9010/2    0.009    0.000    0.182    0.091 rlmeta.py:21(_and)\n     8460    0.006    0.000    0.006    0.000 rlmeta.py:229(__init__)\n    10540    0.005    0.000    0.011    0.000 rlmeta.py:253(peek)\n     1443    0.005    0.000    0.008    0.000 rlmeta.py:163(write)\n    10993    0.004    0.000    0.005    0.000 rlmeta.py:258(is_at_end)\n    885/5    0.004    0.000    0.182    0.036 rlmeta.py:27(_star)\n     8460    0.004    0.000    0.025    0.000 rlmeta.py:250(fail)\n 3274/747    0.003    0.000    0.005    0.000 rlmeta.py:143(create)\n     1802    0.003    0.000    0.011    0.000 rlmeta.py:59(_match_range)\n     2574    0.003    0.000    0.006    0.000 rlmeta.py:271(advance)\n    17881    0.002    0.000    0.002    0.000 rlmeta.py:268(position)\n    12616    0.002    0.000    0.002    0.000 {method 'write' of 'cStringIO.StringO' objects}\n     2575    0.002    0.000    0.004    0.000 rlmeta.py:263(__init__)\n     6225    0.002    0.000    0.002    0.000 {method 'format' of 'str' objects}\n...\n",
"type":"code"
}
],
"id":"20e97823102d44eebb5e45a8cb796cd1",
"type":"code"
}
],
"title":"Peek"
},
{
"children":[],
"id":"84d9e890742c41d48fc1afc2009815c7",
"paragraphs":[
{
"fragments":[
{
"text":"The complete diff can be shown ",
"type":"text"
},
{
"text":"viewed online",
"type":"link",
"url":"https://github.com/rickardlindberg/rickardlindberg.me/commit/079d437dee27cca6cf7aa18d4fffdb5b2dd26172#diff-f500390afaa13cfd96d938b9065dc0c1"
},
{
"text":".",
"type":"text"
}
],
"id":"a84b0514ddda4806a8ac1804235abe47",
"type":"text"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ time ./compile.sh rlmeta.py > /dev/null\n\nreal\t0m0.344s\nuser\t0m0.312s\nsys\t0m0.030s\n",
"type":"code"
}
],
"id":"75570d667bf9476dba654f518e07b538",
"type":"code"
},
{
"chunkpath":[],
"filepath":[],
"fragments":[
{
"text":"$ python -m cProfile -s tottime rlmeta.py < parser.rlmeta\n...\n         240951 function calls (204089 primitive calls) in 0.185 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n  11129/2    0.019    0.000    0.180    0.090 rlmeta.py:48(_match_rule)\n 4177/252    0.017    0.000    0.175    0.001 rlmeta.py:12(_or)\n     6949    0.011    0.000    0.017    0.000 rlmeta.py:221(fail)\n     7021    0.010    0.000    0.038    0.000 rlmeta.py:79(_match_charseq)\n   9010/2    0.010    0.000    0.180    0.090 rlmeta.py:21(_and)\n    10540    0.005    0.000    0.011    0.000 rlmeta.py:253(peek)\n     1443    0.005    0.000    0.008    0.000 rlmeta.py:163(write)\n     6949    0.004    0.000    0.004    0.000 rlmeta.py:229(__init__)\n    10993    0.004    0.000    0.005    0.000 rlmeta.py:258(is_at_end)\n    885/5    0.004    0.000    0.180    0.036 rlmeta.py:27(_star)\n...\n",
"type":"code"
}
],
"id":"8e730042aefa46a29692dd8e8f2afbed",
"type":"code"
},
{
"fragments":[
{
"text":"Can't really tell if better or worse. But should reduce one exception catch/trow. And I like this code better.",
"type":"text"
}
],
"id":"806aff9cb25247c5a37cea9113ca48ca",
"type":"text"
}
],
"title":"No exception last or"
},
{
"children":[],
"id":"3616eda03f5c40458ac9439def097739",
"paragraphs":[
{
"fragments":[
{
"text":"Overall performance roughly doubled.",
"type":"text"
}
],
"id":"bf140dd877764128addd0411f7210bab",
"type":"text"
},
{
"fragments":[
{
"text":"Performance measurements were not very detailed. A bigger grammar could have helped.",
"type":"text"
}
],
"id":"1034d5b114154a9986a261da947633d6",
"type":"text"
},
{
"fragments":[
{
"text":"Only focus on the 20% of the optimizations that were really significant?",
"type":"text"
}
],
"id":"eb7ab3ad65694d5f8ae8c553e0c3546c",
"type":"text"
},
{
"fragments":[
{
"text":"Pick only top 3 optimizations from this article and observe performance gain?",
"type":"text"
}
],
"id":"9b986a3856814e10ab93d538aa26daab",
"type":"text"
}
],
"title":"Conclusions"
},
{
"children":[
{
"children":[],
"id":"496100180e1c4c119f90be842c1b9577",
"paragraphs":[
{
"chunkpath":[],
"filepath":[
"parser.rlmeta"
],
"fragments":[
{
"text":"Parser {\n  grammar =\n    | name:x space '{' rule*:ys space '}'      -> [\"Grammar\" x ~ys]\n  rule =\n    | name:x space '=' choice:y                -> [\"Rule\" x y]\n  choice =\n    | (space '|')?\n      sequence:x (space '|' sequence)*:xs      -> [\"Or\" x ~xs]\n  sequence =\n    | expr:x expr*:xs                          -> [\"Scope\" [\"And\" x ~xs]]\n  expr =\n    | expr1:x space ':' name:y                 -> [\"Bind\" y x]\n    | expr1\n  expr1 =\n    | expr2:x space '*'                        -> [\"Star\" x]\n    | expr2:x space '?'                        -> [\"Or\" x [\"And\"]]\n    | space '!' expr2:x                        -> [\"Not\" x]\n    | space '%'                                -> [\"MatchCallRule\"]\n    | expr2\n  expr2 =\n    | space '->' hostExpr:x                    -> [\"SemanticAction\" x]\n    | name:x !(space '=')                      -> [\"MatchRule\" x]\n    | space char:x '-' char:y                  -> [\"MatchRange\" x y]\n    | space string:x                           -> [\"MatchString\" x]\n    | space charseq:x                          -> [\"MatchCharseq\" x]\n    | space '.'                                -> [\"MatchAny\"]\n    | space '(' choice:x space ')'             -> x\n    | space '[' expr*:xs space ']'             -> [\"MatchList\" [\"And\" ~xs]]\n  hostExpr =\n    | space string:x                           -> [\"String\" x]\n    | space '[' hostExprListItem*:xs space ']' -> [\"List\" ~xs]\n    | space '{' buildExpr*:xs space '}'        -> [\"Builder\" ~xs]\n    | name:x space '(' hostExpr*:ys space ')'  -> [\"FnCall\" x ~ys]\n    | name:x                                   -> [\"VarLookup\" x]\n  hostExprListItem =\n    | space '~' hostExpr:x                     -> [\"ListItemSplice\" x]\n    | hostExpr\n  buildExpr =\n    | space '>'                                -> [\"IndentBuilder\"]\n    | space '<'                                -> [\"DedentBuilder\"]\n    | hostExpr\n  string    = '\"'  (!'\"'  innerChar)*:xs '\"'   -> join(xs)\n  charseq   = '\\'' (!'\\'' innerChar)*:xs '\\''  -> join(xs)\n  char      = '\\''  !'\\'' innerChar  :x  '\\''  -> x\n  innerChar = '\\\\' escape | .\n  escape    = '\\\\' -> \"\\\\\" | '\\'' -> \"'\"\n            | '\"'  -> \"\\\"\" | 'n'  -> \"\\n\"\n  name      = space nameStart:x nameChar*:xs   -> join([x ~xs])\n  nameStart = 'a'-'z' | 'A'-'Z'\n  nameChar  = 'a'-'z' | 'A'-'Z' | '0'-'9'\n  space     = (' ' | '\\n')*\n}\n",
"type":"code"
}
],
"id":"8a67addcac4d443795a7ce297f8c7232",
"language":"",
"post_process":[],
"type":"code"
}
],
"title":"parser.rlmeta"
},
{
"children":[],
"id":"7bd8846187ad478aa17439370f0da1e6",
"paragraphs":[
{
"chunkpath":[],
"filepath":[
"codegenerator.rlmeta"
],
"fragments":[
{
"text":"CodeGenerator {\n  Grammar        = .:x ast*:ys     -> { \"class \" x \"(_Grammar):\\n\" > ys <                       }\n  Rule           = .:x ast:y       -> { \"\\ndef _rule_\" x \"(self):\\n\" > \"return \" y \"()\\n\" <     }\n  MatchAny       =                 -> { \"self._match_any\"                                       }\n  MatchCallRule  =                 -> { \"self._match_call_rule\"                                 }\n  String         = .:x             -> { repr(x)                                                 }\n  List           = astList:x       -> { x                                                       }\n  Builder        = astItems:x      -> { \"_Builder.create([\" x \"])\"                              }\n  IndentBuilder  =                 -> { \"_IndentBuilder()\"                                      }\n  DedentBuilder  =                 -> { \"_DedentBuilder()\"                                      }\n  FnCall         = .:x astItems:y  -> { x \"(\" y \")\"                                             }\n  VarLookup      = .:x             -> { \"_vars.lookup(\" repr(x) \").eval()\"                      }\n  Or             =\n    | ast:x !.                     -> x\n    | astItems:x                   -> { \"(lambda: self._or([\" x \"]))\"                           }\n  Scope          = ast:x           -> { \"(lambda: (lambda _vars:\\n\" > x < \"()\\n)(_Vars()))\"     }\n  And            =\n    | ast:x !.                     -> x\n    | astItems:x                   -> { \"(lambda: self._and([\" x \"]))\"                          }\n  Bind           = .:x ast:y       -> { \"(lambda: _vars.bind(\" repr(x) \", \" y \"()))\"            }\n  Star           = ast:x           -> { \"(lambda: self._star(\" x \"))\"                           }\n  Not            = ast:x           -> { \"(lambda: self._not(\" x \"))\"                            }\n  SemanticAction = ast:x           -> { \"(lambda: _SemanticAction(lambda: \" x \"))\"              }\n  MatchRule      = .:x             -> { \"(lambda: self._match_rule(\" repr(x) \"))\"               }\n  MatchRange     = .:x .:y         -> { \"(lambda: self._match_range(\" repr(x) \", \" repr(y) \"))\" }\n  MatchString    = .:x             -> { \"(lambda: self._match_string(\" repr(x) \"))\"             }\n  MatchCharseq   = .:x             -> { \"(lambda: self._match_charseq(\" repr(x) \"))\"            }\n  MatchList      = ast:x           -> { \"(lambda: self._match_list(\" x \"))\"                     }\n  ast            = [%:x]           -> x\n  astItems       = astItem*:xs     -> { \"\\n\" > xs <                                             }\n  astItem        = ast:x           -> { x \",\\n\"                                                 }\n  astList        = astListItem*:xs -> { \"(\" xs \"[])\"                                            }\n  astListItem    =\n    | [\"ListItemSplice\" ast:x]     -> {     x  \"+\"                                              }\n    | ast:x                        -> { \"[\" x \"]+\"                                              }\n}\n",
"type":"code"
}
],
"id":"196fc11ef11c47468ccf0a093e338906",
"language":"",
"type":"code"
}
],
"title":"codegenerator.rlmeta"
},
{
"children":[],
"id":"ecfe26a58495420b8871e522c6859a4a",
"paragraphs":[
{
"chunkpath":[],
"filepath":[
"support.py"
],
"fragments":[
{
"text":"try:\n    from cStringIO import StringIO\nexcept:\n    from StringIO import StringIO\n\nclass _Grammar(object):\n\n    def _or(self, matchers):\n        original_stream = self._stream\n        for matcher in matchers[:-1]:\n            try:\n                return matcher()\n            except _MatchError:\n                self._stream = original_stream\n        return matchers[-1]()\n\n    def _and(self, matchers):\n        result = None\n        for matcher in matchers:\n            result = matcher()\n        return result\n\n    def _star(self, matcher):\n        result = []\n        while True:\n            original_stream = self._stream\n            try:\n                result.append(matcher())\n            except _MatchError:\n                self._stream = original_stream\n                return _SemanticAction(lambda: [x.eval() for x in result])\n\n    def _not(self, matcher):\n        original_stream = self._stream\n        try:\n            matcher()\n        except _MatchError:\n            return _SemanticAction(lambda: None)\n        else:\n            original_stream.fail(lambda: \"match found\")\n        finally:\n            self._stream = original_stream\n\n    def _match_rule(self, rule_name):\n        key = (rule_name, self._stream.position())\n        if key in self._memo:\n            result, _, self._stream = self._memo[key]\n        else:\n            start = self._stream\n            result = getattr(self, \"_rule_{}\".format(rule_name))()\n            end = self._stream\n            self._memo[key] = (result, start, end)\n        return result\n\n    def _match_range(self, start, end):\n        next_objext = self._stream.peek()\n        if next_objext >= start and next_objext <= end:\n            self._stream = self._stream.advance()\n            return _SemanticAction(lambda: next_objext)\n        else:\n            self._stream.fail(\n                lambda: \"expected range {!r}-{!r} but found {!r}\".format(start, end, next_objext)\n            )\n\n    def _match_string(self, string):\n        next_object = self._stream.peek()\n        if next_object == string:\n            self._stream = self._stream.advance()\n            return _SemanticAction(lambda: string)\n        else:\n            self._stream.fail(\n                lambda: \"expected {!r} but found {!r}\".format(string, next_object)\n            )\n\n    def _match_charseq(self, charseq):\n        for char in charseq:\n            next_object = self._stream.peek()\n            if next_object != char:\n                self._stream.fail(\n                    lambda: \"expected {!r} but found {!r}\".format(char, next_object)\n                )\n            self._stream = self._stream.advance()\n        return _SemanticAction(lambda: charseq)\n\n    def _match_any(self):\n        next_object = self._stream.peek()\n        self._stream = self._stream.advance()\n        return _SemanticAction(lambda: next_object)\n\n    def _match_call_rule(self):\n        next_object = self._stream.peek()\n        self._stream = self._stream.advance()\n        return self._match_rule(str(next_object))\n\n    def _match_list(self, matcher):\n        original_stream = self._stream\n        next_object = self._stream.peek()\n        if isinstance(next_object, list):\n            self._stream = self._stream.nested(next_object)\n            matcher()\n            if self._stream.is_at_end():\n                self._stream = original_stream.advance()\n                return _SemanticAction(lambda: next_object)\n        original_stream.fail(lambda: \"list match failed\")\n\n    def run(self, rule_name, input_object):\n        self._memo = _Memo()\n        self._stream = _Stream.from_object(self._memo, input_object)\n        result = self._match_rule(rule_name).eval()\n        if isinstance(result, _Builder):\n            return result.build_string()\n        else:\n            return result\n\nclass _Vars(dict):\n\n    def bind(self, name, value):\n        self[name] = value\n        return value\n\n    def lookup(self, name):\n        return self[name]\n\nclass _SemanticAction(object):\n\n    def __init__(self, fn):\n        self.fn = fn\n\n    def eval(self):\n        return self.fn()\n\nclass _Builder(object):\n\n    def build_string(self):\n        output = _Output()\n        self.write(output)\n        return output.value\n\n    @classmethod\n    def create(self, item):\n        if isinstance(item, _Builder):\n            return item\n        elif isinstance(item, list):\n            return _ListBuilder([_Builder.create(x) for x in item])\n        else:\n            return _AtomBuilder(item)\n\nclass _Output(object):\n\n    def __init__(self):\n        self.buffer = StringIO()\n        self.indentation = 0\n        self.on_newline = True\n\n    @property\n    def value(self):\n        return self.buffer.getvalue()\n\n    def write(self, value):\n        for ch in value:\n            is_linebreak = ch == \"\\n\"\n            if self.indentation and self.on_newline and not is_linebreak:\n                self.buffer.write(\"    \"*self.indentation)\n            self.buffer.write(ch)\n            self.on_newline = is_linebreak\n\nclass _ListBuilder(_Builder):\n\n    def __init__(self, builders):\n        self.builders = builders\n\n    def write(self, output):\n        for builder in self.builders:\n            builder.write(output)\n\nclass _AtomBuilder(_Builder):\n\n    def __init__(self, atom):\n        self.atom = atom\n\n    def write(self, output):\n        output.write(str(self.atom))\n\nclass _IndentBuilder(_Builder):\n\n    def write(self, output):\n        output.indentation += 1\n\nclass _DedentBuilder(_Builder):\n\n    def write(self, output):\n        output.indentation -= 1\n\nclass _Memo(dict):\n\n    def __init__(self):\n        dict.__init__(self)\n        self._latest_stream = _ObjectStream(self, [], -1)\n        self._latest_lazy_message = lambda: \"\"\n\n    def describe(self):\n        items = []\n        for (rule_name, _), (_, start, end) in self.items():\n            if end > start:\n                items.append((rule_name, start, end))\n        items.sort(key=lambda item: (item[2].position(), item[1].position()))\n        message = []\n        for item in items:\n            message.append(\"matched {: <20} {} -> {}\\n\".format(*item))\n        message.append(\"\\n\")\n        message.append(\"ERROR: {}: {}\\n\".format(\n            self._latest_stream,\n            self._latest_lazy_message()\n        ))\n        return \"\".join(message)\n\n    def fail(self, stream, lazy_message):\n        if stream.position() >= self._latest_stream.position():\n            self._latest_stream = stream\n            self._latest_lazy_message = lazy_message\n        raise _MatchError(self)\n\nclass _MatchError(Exception):\n\n    def __init__(self, memo):\n        Exception.__init__(self)\n        self._memo = memo\n\n    def describe(self):\n        return self._memo.describe()\n\nclass _Stream(object):\n\n    @classmethod\n    def from_object(cls, memo, input_object):\n        if isinstance(input_object, basestring):\n            return _CharStream(memo, input_object, 0)\n        else:\n            return _ObjectStream(memo, [input_object], 0)\n\n    def __init__(self, memo, objects, index):\n        self._memo = memo\n        self._objects = objects\n        self._index = index\n\n    def fail(self, lazy_message):\n        self._memo.fail(self, lazy_message)\n\n    def peek(self):\n        if self.is_at_end():\n            self.fail(lambda: \"not eof\")\n        return self._objects[self._index]\n\n    def is_at_end(self):\n        return self._index >= len(self._objects)\n\nclass _CharStream(_Stream):\n\n    def __init__(self, memo, objects, index, line=1, column=1):\n        _Stream.__init__(self, memo, objects, index)\n        self._line = line\n        self._column = column\n\n    def position(self):\n        return self._index\n\n    def advance(self):\n        if self._objects[self._index] == \"\\n\":\n            line = self._line + 1\n            column = 1\n        else:\n            line = self._line\n            column = self._column + 1\n        return _CharStream(self._memo, self._objects, self._index+1, line, column)\n\n    def __str__(self):\n        return \"L{:03d}:C{:03d}\".format(self._line, self._column)\n\nclass _ObjectStream(_Stream):\n\n    def __init__(self, memo, objects, index, parent=()):\n        _Stream.__init__(self, memo, objects, index)\n        self._parent_position = parent\n        self._position = self._parent_position + (self._index,)\n\n    def position(self):\n        return self._position\n\n    def nested(self, input_object):\n        return _ObjectStream(self._memo, input_object, 0, self._position)\n\n    def advance(self):\n        return _ObjectStream(self._memo, self._objects, self._index+1, self._parent_position)\n\n    def __str__(self):\n        return \"[{}]\".format(\", \".join(str(x) for x in self.position()))\n",
"type":"code"
}
],
"id":"42f83fae0f734a9a9248859c70bd285c",
"type":"code"
}
],
"title":"support.py"
},
{
"children":[],
"id":"1ee923adb0ed4f3b8a5cc72700aad2dd",
"paragraphs":[
{
"code_id":"6299de61907642ed92cffaa3764f2c61",
"id":"b639c030e2a243cebc7ae78c556db96d",
"type":"expanded_code"
}
],
"title":"compile.sh"
},
{
"children":[],
"id":"d09466ef920844788d5e63457decfd86",
"paragraphs":[
{
"chunkpath":[],
"filepath":[
"meta_compile.sh"
],
"fragments":[
{
"text":"#!/bin/bash\n\nset -e\n\ncd \"$(dirname \"$0\")\"\n\n./compile.sh rlmeta.py > rlmeta1.py\n\n./compile.sh rlmeta1.py > rlmeta2.py\n\n./compile.sh rlmeta2.py > rlmeta3.py\n\ndiff rlmeta2.py rlmeta3.py\n\ndiff support.py <(python rlmeta3.py --support)\n\nmv rlmeta3.py rlmeta2.py\n\nmv rlmeta2.py rlmeta1.py\n\nmv rlmeta1.py rlmeta.py\n\necho OK\n",
"type":"code"
}
],
"id":"471910823d0c46e2be98600df125bd34",
"type":"code"
}
],
"title":"meta_compile.sh"
}
],
"id":"3aafb64996414339b01b0d407e6e1810",
"paragraphs":[],
"title":"Code listings for RLMeta"
}
],
"id":"89a873850f674da6a812ca31b0b943d6",
"paragraphs":[
{
"fragments":[
{
"text":"In this article we optimize ",
"type":"text"
},
{
"text":"RLMeta",
"type":"link",
"url":"/writing/rlmeta"
},
{
"text":" to run faster. The first version did not take performance into account, so it is not surprising that we will get it to run twice as fast.",
"type":"text"
}
],
"id":"51307b1cf6714bcc822cd0c169e8f3de",
"type":"text"
}
],
"title":"Optimizing RLMeta"
},
"variables":{}
}